[{"path":"https://marklhc.github.io/plavaan/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Hok Chio (Mark) Lai Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://marklhc.github.io/plavaan/articles/approximate-invariance.html","id":"compared-to-results-from-mplus-version-9-0","dir":"Articles","previous_headings":"","what":"Compared to Results from Mplus (version 9.0)","title":"Approximate Invariance with Penalized Estimation","text":"","code":"# export to Mplus write.table(PoliticalDemocracy,             file = \"inst/mplus/PoliticalDemocracy.dat\",             row.names = FALSE, col.names = FALSE) # Mplus syntax mplus_syntax <- \" TITLE:  Penalized invariance CFA model; DATA:   FILE = PoliticalDemocracy.dat; VARIABLE:         NAMES = y1-y8;         USEVARIABLES = y1-y6 y8; MODEL:  dem60 BY y1-y4* (l1_1-l1_4);         dem65 BY y5-y6* (l2_1-l2_2)                  y8* (l2_4);         dem60@1;         dem65*1;         [dem60@0];         [dem65*0];         [y1-y4] (i1_1-i1_4);         [y5-y6] (i2_1-i2_2);         [y8] (i2_4);         y1 WITH y5;         y2 WITH y6;         y4 WITH y8; MODEL PRIOR:         DO(#,1,2) DIFF(l1_# , l2_#) ~ ALF(0,1);         DO(#,1,2) DIFF(i1_# , i2_#) ~ ALF(0,1);         DIFF(l1_4 , l2_4) ~ ALF(0,1);         DIFF(i1_4 , i2_4) ~ ALF(0,1); \" # Save Mplus syntax writeLines(mplus_syntax, con = \"inst/mplus/penalized_invariance.inp\") # Run Mplus system(\"cd inst/mplus && mplus penalized_invariance.inp\") MODEL RESULTS                                                      Two-Tailed                     Estimate       S.E.  Est./S.E.    P-Value   DEM60    BY     Y1                 2.097      0.212      9.876      0.000     Y2                 2.970      0.285     10.439      0.000     Y3                 2.260      0.323      6.987      0.000     Y4                 2.970      0.220     13.529      0.000   DEM65    BY     Y5                 2.084      0.224      9.315      0.000     Y6                 2.965      0.289     10.250      0.000     Y8                 2.986      0.222     13.470      0.000   DEM65    WITH     DEM60              0.872      0.056     15.638      0.000   Y1       WITH     Y5                 0.941      0.454      2.073      0.038   Y2       WITH     Y6                 1.745      0.899      1.940      0.052   Y4       WITH     Y8                 0.249      0.504      0.494      0.621   Means     DEM60              0.000      0.000    999.000    999.000     DEM65             -0.153      0.074     -2.071      0.038   Intercepts     Y1                 5.461      0.291     18.749      0.000     Y2                 4.220      0.454      9.288      0.000     Y3                 6.563      0.376     17.444      0.000     Y4                 4.472      0.377     11.865      0.000     Y5                 5.459      0.291     18.786      0.000     Y6                 3.446      0.455      7.571      0.000     Y8                 4.480      0.377     11.880      0.000   Variances     DEM60              1.000      0.000    999.000    999.000     DEM65              0.869      0.094      9.207      0.000   Residual Variances     Y1                 2.203      0.519      4.243      0.000     Y2                 6.470      1.338      4.836      0.000     Y3                 5.513      1.088      5.069      0.000     Y4                 2.465      0.627      3.930      0.000     Y5                 3.007      0.617      4.877      0.000     Y6                 3.741      0.801      4.668      0.000     Y8                 2.421      0.711      3.406      0.001"},{"path":"https://marklhc.github.io/plavaan/articles/penalized-cat.html","id":"prepare-ordinal-data","dir":"Articles","previous_headings":"","what":"Prepare Ordinal Data","title":"Penalized Estimation with Ordinal Data and Multiple Groups","text":"First, convert continuous variables ordinal 3 categories:","code":"# Select the 9 cognitive test variables hs_data <- HolzingerSwineford1939[, c(\"school\", \"x1\", \"x2\", \"x3\", \"x4\",                                         \"x5\", \"x6\", \"x7\", \"x8\", \"x9\")]  # Convert to ordinal with 3 points for (i in 2:10) {     hs_data[[i]] <- cut(         hs_data[[i]],          breaks = 3,          labels = FALSE,         include.lowest = TRUE     )     hs_data[[i]] <- ordered(hs_data[[i]]) }  head(hs_data) #>    school x1 x2 x3 x4 x5 x6 x7 x8 x9 #> 1 Pasteur  2  3  1  2  3  1  2  2  2 #> 2 Pasteur  2  2  2  1  1  1  2  2  3 #> 3 Pasteur  2  2  2  1  1  1  1  1  1 #> 4 Pasteur  2  3  2  2  2  2  1  1  1 #> 5 Pasteur  2  2  1  2  2  2  2  2  2 #> 6 Pasteur  2  2  2  1  1  1  2  2  3"},{"path":"https://marklhc.github.io/plavaan/articles/penalized-cat.html","id":"multiple-group-cfa-model","dir":"Articles","previous_headings":"","what":"Multiple-Group CFA Model","title":"Penalized Estimation with Ordinal Data and Multiple Groups","text":"Now fit model across two schools: One also consider magnitude objective function DWLS estimator, scaled differently ML-based functions generally smaller based experience.","code":"mod_base <- \"   visual =~ x1 + x2 + x3   textual =~ x4 + x5 + x6   speed =~ x7 + x8 + x9   x1 ~~ 1 * x1   x2 ~~ 1 * x2   x3 ~~ 1 * x3   x4 ~~ 1 * x4   x5 ~~ 1 * x5   x6 ~~ 1 * x6   x7 ~~ 1 * x7   x8 ~~ 1 * x8   x9 ~~ 1 * x9 \" fit_mg <- cfa(mod_base, data = hs_data, ordered = TRUE, std.lv = TRUE,               parameterization = \"theta\", group = \"school\") summary(fit_mg, fit.measures = TRUE) #> lavaan 0.6-20 ended normally after 179 iterations #>  #>   Estimator                                       DWLS #>   Optimization method                           NLMINB #>   Number of model parameters                        60 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #> Model Test User Model: #>                                               Standard      Scaled #>   Test Statistic                                71.775      99.182 #>   Degrees of freedom                                48          48 #>   P-value (Chi-square)                           0.015       0.000 #>   Scaling correction factor                                  0.798 #>   Shift parameter                                            9.213 #>     simple second-order correction                                 #>   Test statistic for each group: #>     Pasteur                                     56.128      56.128 #>     Grant-White                                 43.055      43.055 #>  #> Model Test Baseline Model: #>  #>   Test statistic                              1706.441    1175.245 #>   Degrees of freedom                                72          72 #>   P-value                                        0.000       0.000 #>   Scaling correction factor                                  1.481 #>  #> User Model versus Baseline Model: #>  #>   Comparative Fit Index (CFI)                    0.985       0.954 #>   Tucker-Lewis Index (TLI)                       0.978       0.930 #>                                                                    #>   Robust Comparative Fit Index (CFI)                         0.863 #>   Robust Tucker-Lewis Index (TLI)                            0.794 #>  #> Root Mean Square Error of Approximation: #>  #>   RMSEA                                          0.058       0.084 #>   90 Percent confidence interval - lower         0.026       0.061 #>   90 Percent confidence interval - upper         0.084       0.108 #>   P-value H_0: RMSEA <= 0.050                    0.308       0.011 #>   P-value H_0: RMSEA >= 0.080                    0.084       0.641 #>                                                                    #>   Robust RMSEA                                               0.143 #>   90 Percent confidence interval - lower                     0.098 #>   90 Percent confidence interval - upper                     0.187 #>   P-value H_0: Robust RMSEA <= 0.050                         0.001 #>   P-value H_0: Robust RMSEA >= 0.080                         0.987 #>  #> Standardized Root Mean Square Residual: #>  #>   SRMR                                           0.087       0.087 #>  #> Parameter Estimates: #>  #>   Parameterization                               Theta #>   Standard errors                           Robust.sem #>   Information                                 Expected #>   Information saturated (h1) model        Unstructured #>  #>  #> Group 1 [Pasteur]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   visual =~                                            #>     x1                2.581    2.738    0.943    0.346 #>     x2                0.536    0.162    3.315    0.001 #>     x3                0.683    0.177    3.866    0.000 #>   textual =~                                           #>     x4                1.504    0.340    4.424    0.000 #>     x5                2.586    1.010    2.560    0.010 #>     x6                1.766    0.443    3.988    0.000 #>   speed =~                                             #>     x7                0.727    0.243    2.992    0.003 #>     x8                0.938    0.353    2.654    0.008 #>     x9                0.792    0.280    2.830    0.005 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   visual ~~                                            #>     textual           0.426    0.099    4.292    0.000 #>     speed             0.187    0.113    1.654    0.098 #>   textual ~~                                           #>     speed             0.293    0.107    2.738    0.006 #>  #> Thresholds: #>                    Estimate  Std.Err  z-value  P(>|z|) #>     x1|t1            -4.074    3.773   -1.080    0.280 #>     x1|t2             2.407    2.214    1.087    0.277 #>     x2|t1            -1.523    0.181   -8.415    0.000 #>     x2|t2             0.909    0.138    6.586    0.000 #>     x3|t1            -0.699    0.139   -5.011    0.000 #>     x3|t2             0.522    0.130    3.998    0.000 #>     x4|t1            -0.940    0.231   -4.077    0.000 #>     x4|t2             2.106    0.376    5.594    0.000 #>     x5|t1            -1.444    0.555   -2.601    0.009 #>     x5|t2             2.042    0.759    2.690    0.007 #>     x6|t1             0.874    0.287    3.045    0.002 #>     x6|t2             3.956    0.801    4.941    0.000 #>     x7|t1            -1.330    0.211   -6.309    0.000 #>     x7|t2             1.018    0.179    5.683    0.000 #>     x8|t1            -0.110    0.139   -0.793    0.428 #>     x8|t2             2.538    0.502    5.054    0.000 #>     x9|t1            -0.505    0.146   -3.465    0.001 #>     x9|t2             2.164    0.352    6.155    0.000 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .x1                1.000                            #>    .x2                1.000                            #>    .x3                1.000                            #>    .x4                1.000                            #>    .x5                1.000                            #>    .x6                1.000                            #>    .x7                1.000                            #>    .x8                1.000                            #>    .x9                1.000                            #>     visual            1.000                            #>     textual           1.000                            #>     speed             1.000                            #>  #> Scales y*: #>                    Estimate  Std.Err  z-value  P(>|z|) #>     x1                0.361                            #>     x2                0.881                            #>     x3                0.826                            #>     x4                0.554                            #>     x5                0.361                            #>     x6                0.493                            #>     x7                0.809                            #>     x8                0.730                            #>     x9                0.784                            #>  #>  #> Group 2 [Grant-White]: #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   visual =~                                            #>     x1                1.149    0.383    2.997    0.003 #>     x2                0.575    0.151    3.812    0.000 #>     x3                0.802    0.184    4.370    0.000 #>   textual =~                                           #>     x4                1.629    0.358    4.551    0.000 #>     x5                2.303    0.698    3.299    0.001 #>     x6                1.166    0.207    5.632    0.000 #>   speed =~                                             #>     x7                0.977    0.218    4.492    0.000 #>     x8                0.995    0.231    4.314    0.000 #>     x9                3.035    2.806    1.082    0.279 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   visual ~~                                            #>     textual           0.596    0.086    6.964    0.000 #>     speed             0.553    0.108    5.100    0.000 #>   textual ~~                                           #>     speed             0.423    0.091    4.661    0.000 #>  #> Thresholds: #>                    Estimate  Std.Err  z-value  P(>|z|) #>     x1|t1            -1.923    0.406   -4.736    0.000 #>     x1|t2             1.245    0.293    4.242    0.000 #>     x2|t1            -2.212    0.258   -8.564    0.000 #>     x2|t2             0.784    0.136    5.769    0.000 #>     x3|t1            -0.167    0.135   -1.239    0.215 #>     x3|t2             1.111    0.170    6.522    0.000 #>     x4|t1            -2.272    0.413   -5.502    0.000 #>     x4|t2             1.471    0.312    4.716    0.000 #>     x5|t1            -3.168    0.850   -3.726    0.000 #>     x5|t2             0.817    0.331    2.465    0.014 #>     x6|t1            -0.093    0.161   -0.579    0.563 #>     x6|t2             1.939    0.274    7.063    0.000 #>     x7|t1            -0.832    0.177   -4.711    0.000 #>     x7|t2             1.820    0.263    6.908    0.000 #>     x8|t1            -0.257    0.151   -1.707    0.088 #>     x8|t2             2.705    0.421    6.430    0.000 #>     x9|t1            -1.458    1.266   -1.151    0.250 #>     x9|t2             6.519    5.519    1.181    0.238 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>    .x1                1.000                            #>    .x2                1.000                            #>    .x3                1.000                            #>    .x4                1.000                            #>    .x5                1.000                            #>    .x6                1.000                            #>    .x7                1.000                            #>    .x8                1.000                            #>    .x9                1.000                            #>     visual            1.000                            #>     textual           1.000                            #>     speed             1.000                            #>  #> Scales y*: #>                    Estimate  Std.Err  z-value  P(>|z|) #>     x1                0.656                            #>     x2                0.867                            #>     x3                0.780                            #>     x4                0.523                            #>     x5                0.398                            #>     x6                0.651                            #>     x7                0.715                            #>     x8                0.709                            #>     x9                0.313 fit_mg@optim$fx #> [1] 0.1192268"},{"path":"https://marklhc.github.io/plavaan/articles/penalized-cat.html","id":"strict-and-partial-invariance-models","dir":"Articles","previous_headings":"Multiple-Group CFA Model","what":"Strict and partial invariance models","title":"Penalized Estimation with Ordinal Data and Multiple Groups","text":"item 9 showed non-invariant thresholds, based score test.","code":"# Strict invariance: constrain loadings, thresholds, and residual variances fit_strict <- cfa(mod_base, data = hs_data, ordered = TRUE, std.lv = TRUE,                   parameterization = \"theta\", group = \"school\",                   group.equal = c(\"loadings\", \"thresholds\", \"residuals\"))  # Score test lavTestScore(fit_strict) #> Warning: lavaan->lavTestScore():   #>    se is not `standard'; not implemented yet; falling back to ordinary score  #>    test #> $test #>  #> total score test: #>  #>    test     X2 df p.value #> 1 score 18.231 27   0.896 #>  #> $uni #>  #> univariate score tests: #>  #>      lhs op   rhs    X2 df p.value #> 1   .p1. == .p64. 0.345  1   0.557 #> 2   .p2. == .p65. 0.730  1   0.393 #> 3   .p3. == .p66. 0.027  1   0.869 #> 4   .p4. == .p67. 0.110  1   0.740 #> 5   .p5. == .p68. 0.310  1   0.578 #> 6   .p6. == .p69. 0.049  1   0.824 #> 7   .p7. == .p70. 0.995  1   0.319 #> 8   .p8. == .p71. 0.003  1   0.960 #> 9   .p9. == .p72. 0.769  1   0.380 #> 10 .p19. == .p82. 1.308  1   0.253 #> 11 .p20. == .p83. 1.308  1   0.253 #> 12 .p21. == .p84. 2.439  1   0.118 #> 13 .p22. == .p85. 2.439  1   0.118 #> 14 .p23. == .p86. 0.000  1   0.986 #> 15 .p24. == .p87. 0.000  1   0.986 #> 16 .p25. == .p88. 0.082  1   0.774 #> 17 .p26. == .p89. 0.082  1   0.774 #> 18 .p27. == .p90. 0.415  1   0.519 #> 19 .p28. == .p91. 0.415  1   0.519 #> 20 .p29. == .p92. 1.428  1   0.232 #> 21 .p30. == .p93. 1.428  1   0.232 #> 22 .p31. == .p94. 0.158  1   0.691 #> 23 .p32. == .p95. 0.158  1   0.691 #> 24 .p33. == .p96. 0.802  1   0.371 #> 25 .p34. == .p97. 0.802  1   0.371 #> 26 .p35. == .p98. 4.157  1   0.041 #> 27 .p36. == .p99. 4.157  1   0.041"},{"path":"https://marklhc.github.io/plavaan/articles/penalized-cat.html","id":"penalized-multiple-group-model","dir":"Articles","previous_headings":"","what":"Penalized Multiple-Group Model","title":"Penalized Estimation with Ordinal Data and Multiple Groups","text":"’ll penalize differences loadings thresholds across groups. First, set -specified (unidentified) model latent mean variance identified first group, without fitting (.fit = FALSE): Examine parameter table identify loadings thresholds: Identify parameter IDs loadings thresholds group: Fit penalized model penalties differences loadings thresholds:","code":"mod_un <- \"   visual =~ x1 + x2 + x3   textual =~ x4 + x5 + x6   speed =~ x7 + x8 + x9   visual ~~ c(1, NA) * visual   textual ~~ c(1, NA) * textual   speed ~~ c(1, NA) * speed   visual ~ c(0, NA) * 1   textual ~ c(0, NA) * 1   speed ~ c(0, NA) * 1   x1 ~~ 1 * x1   x2 ~~ 1 * x2   x3 ~~ 1 * x3   x4 ~~ 1 * x4   x5 ~~ 1 * x5   x6 ~~ 1 * x6   x7 ~~ 1 * x7   x8 ~~ 1 * x8   x9 ~~ 1 * x9 \" fit_mg_nofit <- cfa(mod_base, data = hs_data, ordered = TRUE, std.lv = TRUE,                     auto.fix.first = FALSE,                     parameterization = \"theta\", group = \"school\", do.fit = FALSE) pt <- parTable(fit_mg_nofit) # Show loadings pt[pt$op == \"=~\", c(\"lhs\", \"op\", \"rhs\", \"group\", \"free\")] #>        lhs op rhs group free #> 1   visual =~  x1     1    1 #> 2   visual =~  x2     1    2 #> 3   visual =~  x3     1    3 #> 4  textual =~  x4     1    4 #> 5  textual =~  x5     1    5 #> 6  textual =~  x6     1    6 #> 7    speed =~  x7     1    7 #> 8    speed =~  x8     1    8 #> 9    speed =~  x9     1    9 #> 64  visual =~  x1     2   31 #> 65  visual =~  x2     2   32 #> 66  visual =~  x3     2   33 #> 67 textual =~  x4     2   34 #> 68 textual =~  x5     2   35 #> 69 textual =~  x6     2   36 #> 70   speed =~  x7     2   37 #> 71   speed =~  x8     2   38 #> 72   speed =~  x9     2   39 # Show thresholds pt[pt$op == \"|\", c(\"lhs\", \"op\", \"rhs\", \"group\", \"free\")] #>    lhs op rhs group free #> 19  x1  |  t1     1   10 #> 20  x1  |  t2     1   11 #> 21  x2  |  t1     1   12 #> 22  x2  |  t2     1   13 #> 23  x3  |  t1     1   14 #> 24  x3  |  t2     1   15 #> 25  x4  |  t1     1   16 #> 26  x4  |  t2     1   17 #> 27  x5  |  t1     1   18 #> 28  x5  |  t2     1   19 #> 29  x6  |  t1     1   20 #> 30  x6  |  t2     1   21 #> 31  x7  |  t1     1   22 #> 32  x7  |  t2     1   23 #> 33  x8  |  t1     1   24 #> 34  x8  |  t2     1   25 #> 35  x9  |  t1     1   26 #> 36  x9  |  t2     1   27 #> 82  x1  |  t1     2   40 #> 83  x1  |  t2     2   41 #> 84  x2  |  t1     2   42 #> 85  x2  |  t2     2   43 #> 86  x3  |  t1     2   44 #> 87  x3  |  t2     2   45 #> 88  x4  |  t1     2   46 #> 89  x4  |  t2     2   47 #> 90  x5  |  t1     2   48 #> 91  x5  |  t2     2   49 #> 92  x6  |  t1     2   50 #> 93  x6  |  t2     2   51 #> 94  x7  |  t1     2   52 #> 95  x7  |  t2     2   53 #> 96  x8  |  t1     2   54 #> 97  x8  |  t2     2   55 #> 98  x9  |  t1     2   56 #> 99  x9  |  t2     2   57 # Loadings: group 1 (Pasteur) and group 2 (Grant-White) load_g1 <- pt$free[pt$op == \"=~\" & pt$group == 1 & pt$free > 0] load_g2 <- pt$free[pt$op == \"=~\" & pt$group == 2 & pt$free > 0]  # Thresholds: group 1 and group 2 thresh_g1 <- pt$free[pt$op == \"|\" & pt$group == 1 & pt$free > 0] thresh_g2 <- pt$free[pt$op == \"|\" & pt$group == 2 & pt$free > 0]  print(list(     loadings_g1 = load_g1,     loadings_g2 = load_g2,     thresholds_g1 = thresh_g1,     thresholds_g2 = thresh_g2 )) #> $loadings_g1 #> [1] 1 2 3 4 5 6 7 8 9 #>  #> $loadings_g2 #> [1] 31 32 33 34 35 36 37 38 39 #>  #> $thresholds_g1 #>  [1] 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 #>  #> $thresholds_g2 #>  [1] 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 fit_pen_mg <- penalized_est(     fit_mg_nofit,     w = 0.03,     pen_diff_id = list(         loadings = rbind(load_g1, load_g2),         thresholds = rbind(thresh_g1, thresh_g2)     ) ) summary(fit_pen_mg) #> lavaan 0.6-20 ended normally after 152 iterations #>  #>   Estimator                                       DWLS #>   Optimization method                           NLMINB #>   Number of model parameters                        60 #>  #>   Number of observations per group:                    #>     Pasteur                                        156 #>     Grant-White                                    145 #>  #>  #> Parameter Estimates: #>  #>   Parameterization                               Theta #>  #>  #> Group 1 [Pasteur]: #>  #> Latent Variables: #>                    Estimate #>   visual =~                 #>     x1                1.435 #>     x2                0.567 #>     x3                0.763 #>   textual =~                #>     x4                1.548 #>     x5                2.499 #>     x6                1.400 #>   speed =~                  #>     x7                0.849 #>     x8                0.930 #>     x9                1.550 #>  #> Covariances: #>                    Estimate #>   visual ~~                 #>     textual           0.448 #>     speed             0.193 #>   textual ~~                #>     speed             0.258 #>  #> Thresholds: #>                    Estimate #>     x1|t1            -2.379 #>     x1|t2             1.477 #>     x2|t1            -1.565 #>     x2|t2             0.853 #>     x3|t1            -0.704 #>     x3|t2             0.566 #>     x4|t1            -0.963 #>     x4|t2             2.116 #>     x5|t1            -1.404 #>     x5|t2             1.970 #>     x6|t1             0.731 #>     x6|t2             3.338 #>     x7|t1            -1.388 #>     x7|t2             1.100 #>     x8|t1            -0.176 #>     x8|t2             2.570 #>     x9|t1            -0.782 #>     x9|t2             3.352 #>  #> Variances: #>                    Estimate #>    .x1                1.000 #>    .x2                1.000 #>    .x3                1.000 #>    .x4                1.000 #>    .x5                1.000 #>    .x6                1.000 #>    .x7                1.000 #>    .x8                1.000 #>    .x9                1.000 #>     visual            1.000 #>     textual           1.000 #>     speed             1.000 #>  #>  #> Group 2 [Grant-White]: #>  #> Latent Variables: #>                    Estimate #>   visual =~                 #>     x1                1.437 #>     x2                0.567 #>     x3                0.763 #>   textual =~                #>     x4                1.548 #>     x5                2.498 #>     x6                1.395 #>   speed =~                  #>     x7                0.852 #>     x8                0.931 #>     x9                1.549 #>  #> Covariances: #>                    Estimate #>   visual ~~                 #>     textual           0.561 #>     speed             0.581 #>   textual ~~                #>     speed             0.448 #>  #> Thresholds: #>                    Estimate #>     x1|t1            -2.378 #>     x1|t2             1.476 #>     x2|t1            -2.153 #>     x2|t2             0.850 #>     x3|t1            -0.184 #>     x3|t2             1.058 #>     x4|t1            -2.185 #>     x4|t2             1.445 #>     x5|t1            -3.393 #>     x5|t2             0.887 #>     x6|t1            -0.094 #>     x6|t2             2.173 #>     x7|t1            -0.800 #>     x7|t2             1.678 #>     x8|t1            -0.178 #>     x8|t2             2.570 #>     x9|t1            -0.783 #>     x9|t2             3.353 #>  #> Variances: #>                    Estimate #>    .x1                1.000 #>    .x2                1.000 #>    .x3                1.000 #>    .x4                1.000 #>    .x5                1.000 #>    .x6                1.000 #>    .x7                1.000 #>    .x8                1.000 #>    .x9                1.000 #>     visual            1.000 #>     textual           1.000 #>     speed             1.000"},{"path":"https://marklhc.github.io/plavaan/articles/penalized-cat.html","id":"evaluate-invariance","dir":"Articles","previous_headings":"","what":"Evaluate Invariance","title":"Penalized Estimation with Ordinal Data and Multiple Groups","text":"estimated loadings thresholds, can calculate effective number parameters differ across groups: penalized estimation approach identifies loadings thresholds substantively differ across groups, providing efficienct, data-driven assessment measurement invariance ordinal data.","code":"# Loadings load_ests_g1 <- as.numeric(coef(fit_pen_mg)[load_g1]) load_ests_g2 <- as.numeric(coef(fit_pen_mg)[load_g2]) load_mat <- rbind(load_ests_g1, load_ests_g2) colnames(load_mat) <- names(coef(fit_pen_mg))[load_g1] eff_load_diff <- composite_pair_loss(load_mat, fun = l0a)  # Thresholds thresh_ests_g1 <- as.numeric(coef(fit_pen_mg)[thresh_g1]) thresh_ests_g2 <- as.numeric(coef(fit_pen_mg)[thresh_g2]) thresh_mat <- rbind(thresh_ests_g1, thresh_ests_g2) colnames(thresh_mat) <- names(coef(fit_pen_mg))[thresh_g1] eff_thresh_diff <- composite_pair_loss(thresh_mat, fun = l0a)  cat(\"Penalized Loading Estimates:\\n\") #> Penalized Loading Estimates: print(load_mat, digits = 3) #>              visual=~x1 visual=~x2 visual=~x3 textual=~x4 textual=~x5 #> load_ests_g1       1.43      0.567      0.763        1.55         2.5 #> load_ests_g2       1.44      0.567      0.763        1.55         2.5 #>              textual=~x6 speed=~x7 speed=~x8 speed=~x9 #> load_ests_g1         1.4     0.849     0.930      1.55 #> load_ests_g2         1.4     0.852     0.931      1.55  cat(\"\\nPenalized Threshold Estimates:\\n\") #>  #> Penalized Threshold Estimates: print(thresh_mat, digits = 3) #>                x1|t1 x1|t2 x2|t1 x2|t2  x3|t1 x3|t2  x4|t1 x4|t2 x5|t1 x5|t2 #> thresh_ests_g1 -2.38  1.48 -1.57 0.853 -0.704 0.566 -0.963  2.12 -1.40 1.970 #> thresh_ests_g2 -2.38  1.48 -2.15 0.850 -0.184 1.058 -2.185  1.45 -3.39 0.887 #>                  x6|t1 x6|t2 x7|t1 x7|t2  x8|t1 x8|t2  x9|t1 x9|t2 #> thresh_ests_g1  0.7309  3.34 -1.39  1.10 -0.176  2.57 -0.782  3.35 #> thresh_ests_g2 -0.0939  2.17 -0.80  1.68 -0.178  2.57 -0.783  3.35  cat(\"Effective number of non-invariant loadings:\", eff_load_diff, \"\\n\") #> Effective number of non-invariant loadings: 0.004100349 cat(\"Effective number of non-invariant thresholds:\", eff_thresh_diff, \"\\n\") #> Effective number of non-invariant thresholds: 10.77962"},{"path":[]},{"path":"https://marklhc.github.io/plavaan/articles/penalized-fa.html","id":"two-factor-cfa-model","dir":"Articles","previous_headings":"Penalize cross-loadings","what":"Two-factor CFA model","title":"Penalized Estimation of Cross-Loadings and Unique Covariances","text":"","code":"mod0 <- \"   ind60 =~ x1 + x2 + x3   dem60 =~ y1 + y2 + y3 + y4   ind60 ~~ dem60 \" fit0 <- cfa(mod0, data = PoliticalDemocracy, std.lv = TRUE)"},{"path":"https://marklhc.github.io/plavaan/articles/penalized-fa.html","id":"two-factor-efa-model-unidentified","dir":"Articles","previous_headings":"Penalize cross-loadings","what":"Two-factor EFA model (unidentified)","title":"Penalized Estimation of Cross-Loadings and Unique Covariances","text":"","code":"mod <- \"   ind60 =~ x1 + x2 + x3 + y1 + y2 + y3 + y4   dem60 =~ x1 + x2 + x3 + y1 + y2 + y3 + y4   ind60 ~~ ind60 \" fit <- cfa(mod, data = PoliticalDemocracy, std.lv = TRUE, do.fit = FALSE)"},{"path":"https://marklhc.github.io/plavaan/articles/penalized-fa.html","id":"two-factor-efa-model-with-penalized-cross-loadings","dir":"Articles","previous_headings":"Penalize cross-loadings","what":"Two-factor EFA model with penalized cross-loadings","title":"Penalized Estimation of Cross-Loadings and Unique Covariances","text":"cross-loadings parameters 4 10 parameter table (see free column).","code":"parTable(fit) #>    id   lhs op   rhs user block group free ustart exo label plabel start   est #> 1   1 ind60 =~    x1    1     1     1    1     NA   0         .p1. 0.951 0.951 #> 2   2 ind60 =~    x2    1     1     1    2     NA   0         .p2. 2.001 2.001 #> 3   3 ind60 =~    x3    1     1     1    3     NA   0         .p3. 1.687 1.687 #> 4   4 ind60 =~    y1    1     1     1    4     NA   0         .p4. 1.344 1.344 #> 5   5 ind60 =~    y2    1     1     1    5     NA   0         .p5. 1.807 1.807 #> 6   6 ind60 =~    y3    1     1     1    6     NA   0         .p6. 1.778 1.778 #> 7   7 ind60 =~    y4    1     1     1    7     NA   0         .p7. 2.256 2.256 #> 8   8 dem60 =~    x1    1     1     1    8     NA   0         .p8. 0.951 0.951 #> 9   9 dem60 =~    x2    1     1     1    9     NA   0         .p9. 2.001 2.001 #> 10 10 dem60 =~    x3    1     1     1   10     NA   0        .p10. 1.687 1.687 #> 11 11 dem60 =~    y1    1     1     1   11     NA   0        .p11. 1.344 1.344 #> 12 12 dem60 =~    y2    1     1     1   12     NA   0        .p12. 1.807 1.807 #> 13 13 dem60 =~    y3    1     1     1   13     NA   0        .p13. 1.778 1.778 #> 14 14 dem60 =~    y4    1     1     1   14     NA   0        .p14. 2.256 2.256 #> 15 15 ind60 ~~ ind60    1     1     1    0      1   0        .p15. 1.000 1.000 #> 16 16    x1 ~~    x1    0     1     1   15     NA   0        .p16. 0.265 0.265 #> 17 17    x2 ~~    x2    0     1     1   16     NA   0        .p17. 1.126 1.126 #> 18 18    x3 ~~    x3    0     1     1   17     NA   0        .p18. 0.975 0.975 #> 19 19    y1 ~~    y1    0     1     1   18     NA   0        .p19. 3.393 3.393 #> 20 20    y2 ~~    y2    0     1     1   19     NA   0        .p20. 7.686 7.686 #> 21 21    y3 ~~    y3    0     1     1   20     NA   0        .p21. 5.310 5.310 #> 22 22    y4 ~~    y4    0     1     1   21     NA   0        .p22. 5.535 5.535 #> 23 23 dem60 ~~ dem60    0     1     1    0      1   0        .p23. 1.000 1.000 #> 24 24 ind60 ~~ dem60    0     1     1   22     NA   0        .p24. 0.000 0.000 pefa_fit <- penalized_est(     fit,     w = .03,     pen_par_id = 4:10 ) summary(pefa_fit) #> lavaan 0.6-20 ended normally after 126 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        22 #>  #>   Number of observations                            75 #>  #>  #> Parameter Estimates: #>  #>  #> Latent Variables: #>                    Estimate #>   ind60 =~                  #>     x1                0.658 #>     x2                1.456 #>     x3                1.222 #>     y1               -0.007 #>     y2               -0.608 #>     y3               -0.001 #>     y4                0.006 #>   dem60 =~                  #>     x1                0.025 #>     x2               -0.002 #>     x3               -0.010 #>     y1                2.071 #>     y2                3.290 #>     y3                2.256 #>     y4                2.999 #>  #> Covariances: #>                    Estimate #>   ind60 ~~                  #>     dem60             0.481 #>  #> Variances: #>                    Estimate #>     ind60             1.000 #>    .x1                0.079 #>    .x2                0.127 #>    .x3                0.464 #>    .y1                2.493 #>    .y2                6.048 #>    .y3                5.512 #>    .y4                2.017 #>     dem60             1.000"},{"path":[]},{"path":"https://marklhc.github.io/plavaan/articles/penalized-fa.html","id":"two-factor-efa-model-with-unique-covariances","dir":"Articles","previous_headings":"Penalize Cross-loadings and Unique Covariances","what":"Two-factor EFA model with unique covariances","title":"Penalized Estimation of Cross-Loadings and Unique Covariances","text":"","code":"mod2 <- \"   ind60 =~ x1 + x2 + x3 + y1 + y2 + y3 + y4   dem60 =~ x1 + x2 + x3 + y1 + y2 + y3 + y4   ind60 ~~ ind60   x1 ~~ x2 + x3 + y1 + y2 + y3 + y4   x2 ~~ x3 + y1 + y2 + y3 + y4   x3 ~~ y1 + y2 + y3 + y4   y1 ~~ y2 + y3 + y4   y2 ~~ y3 + y4   y3 ~~ y4 \" fit2 <- cfa(mod2, data = PoliticalDemocracy, std.lv = TRUE, do.fit = FALSE)"},{"path":"https://marklhc.github.io/plavaan/articles/penalized-fa.html","id":"two-factor-efa-model-with-penalized-cross-loadings-and-unique-covariances","dir":"Articles","previous_headings":"Penalize Cross-loadings and Unique Covariances","what":"Two-factor EFA model with penalized cross-loadings and unique covariances","title":"Penalized Estimation of Cross-Loadings and Unique Covariances","text":"unique covariances parameters 15 35 parameter table (see free column). unique covariances estimated close zero. One can approximate “effective” number cross-loadings unique covariances : 28 parameters penalized, 1.6 (close 2) effectively non-zero.","code":"parTable(fit2) #>    id   lhs op   rhs user block group free ustart exo label plabel start   est #> 1   1 ind60 =~    x1    1     1     1    1     NA   0         .p1. 0.951 0.951 #> 2   2 ind60 =~    x2    1     1     1    2     NA   0         .p2. 2.001 2.001 #> 3   3 ind60 =~    x3    1     1     1    3     NA   0         .p3. 1.687 1.687 #> 4   4 ind60 =~    y1    1     1     1    4     NA   0         .p4. 1.344 1.344 #> 5   5 ind60 =~    y2    1     1     1    5     NA   0         .p5. 1.807 1.807 #> 6   6 ind60 =~    y3    1     1     1    6     NA   0         .p6. 1.778 1.778 #> 7   7 ind60 =~    y4    1     1     1    7     NA   0         .p7. 2.256 2.256 #> 8   8 dem60 =~    x1    1     1     1    8     NA   0         .p8. 0.951 0.951 #> 9   9 dem60 =~    x2    1     1     1    9     NA   0         .p9. 2.001 2.001 #> 10 10 dem60 =~    x3    1     1     1   10     NA   0        .p10. 1.687 1.687 #> 11 11 dem60 =~    y1    1     1     1   11     NA   0        .p11. 1.344 1.344 #> 12 12 dem60 =~    y2    1     1     1   12     NA   0        .p12. 1.807 1.807 #> 13 13 dem60 =~    y3    1     1     1   13     NA   0        .p13. 1.778 1.778 #> 14 14 dem60 =~    y4    1     1     1   14     NA   0        .p14. 2.256 2.256 #> 15 15 ind60 ~~ ind60    1     1     1    0      1   0        .p15. 1.000 1.000 #> 16 16    x1 ~~    x2    1     1     1   15     NA   0        .p16. 0.000 0.000 #> 17 17    x1 ~~    x3    1     1     1   16     NA   0        .p17. 0.000 0.000 #> 18 18    x1 ~~    y1    1     1     1   17     NA   0        .p18. 0.000 0.000 #> 19 19    x1 ~~    y2    1     1     1   18     NA   0        .p19. 0.000 0.000 #> 20 20    x1 ~~    y3    1     1     1   19     NA   0        .p20. 0.000 0.000 #> 21 21    x1 ~~    y4    1     1     1   20     NA   0        .p21. 0.000 0.000 #> 22 22    x2 ~~    x3    1     1     1   21     NA   0        .p22. 0.000 0.000 #> 23 23    x2 ~~    y1    1     1     1   22     NA   0        .p23. 0.000 0.000 #> 24 24    x2 ~~    y2    1     1     1   23     NA   0        .p24. 0.000 0.000 #> 25 25    x2 ~~    y3    1     1     1   24     NA   0        .p25. 0.000 0.000 #> 26 26    x2 ~~    y4    1     1     1   25     NA   0        .p26. 0.000 0.000 #> 27 27    x3 ~~    y1    1     1     1   26     NA   0        .p27. 0.000 0.000 #> 28 28    x3 ~~    y2    1     1     1   27     NA   0        .p28. 0.000 0.000 #> 29 29    x3 ~~    y3    1     1     1   28     NA   0        .p29. 0.000 0.000 #> 30 30    x3 ~~    y4    1     1     1   29     NA   0        .p30. 0.000 0.000 #> 31 31    y1 ~~    y2    1     1     1   30     NA   0        .p31. 0.000 0.000 #> 32 32    y1 ~~    y3    1     1     1   31     NA   0        .p32. 0.000 0.000 #> 33 33    y1 ~~    y4    1     1     1   32     NA   0        .p33. 0.000 0.000 #> 34 34    y2 ~~    y3    1     1     1   33     NA   0        .p34. 0.000 0.000 #> 35 35    y2 ~~    y4    1     1     1   34     NA   0        .p35. 0.000 0.000 #> 36 36    y3 ~~    y4    1     1     1   35     NA   0        .p36. 0.000 0.000 #> 37 37    x1 ~~    x1    0     1     1   36     NA   0        .p37. 0.265 0.265 #> 38 38    x2 ~~    x2    0     1     1   37     NA   0        .p38. 1.126 1.126 #> 39 39    x3 ~~    x3    0     1     1   38     NA   0        .p39. 0.975 0.975 #> 40 40    y1 ~~    y1    0     1     1   39     NA   0        .p40. 3.393 3.393 #> 41 41    y2 ~~    y2    0     1     1   40     NA   0        .p41. 7.686 7.686 #> 42 42    y3 ~~    y3    0     1     1   41     NA   0        .p42. 5.310 5.310 #> 43 43    y4 ~~    y4    0     1     1   42     NA   0        .p43. 5.535 5.535 #> 44 44 dem60 ~~ dem60    0     1     1    0      1   0        .p44. 1.000 1.000 #> 45 45 ind60 ~~ dem60    0     1     1   43     NA   0        .p45. 0.000 0.000 pefa_fit2 <- penalized_est(     fit2,     w = .03,     pen_par_id = c(4:10, 15:35) ) summary(pefa_fit2) #> lavaan 0.6-20 ended normally after 195 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        43 #>  #>   Number of observations                            75 #>  #>  #> Parameter Estimates: #>  #>  #> Latent Variables: #>                    Estimate #>   ind60 =~                  #>     x1                0.665 #>     x2                1.449 #>     x3                1.225 #>     y1                0.003 #>     y2               -0.005 #>     y3                0.003 #>     y4                0.455 #>   dem60 =~                  #>     x1                0.020 #>     x2                0.001 #>     x3               -0.012 #>     y1                2.119 #>     y2                3.018 #>     y3                2.307 #>     y4                2.743 #>  #> Covariances: #>                    Estimate #>  .x1 ~~                     #>    .x2               -0.004 #>    .x3               -0.009 #>    .y1                0.054 #>    .y2               -0.050 #>    .y3                0.001 #>    .y4                0.018 #>  .x2 ~~                     #>    .x3                0.006 #>    .y1               -0.002 #>    .y2                0.005 #>    .y3                0.010 #>    .y4               -0.012 #>  .x3 ~~                     #>    .y1               -0.010 #>    .y2                0.003 #>    .y3               -0.008 #>    .y4                0.006 #>  .y1 ~~                     #>    .y2               -0.002 #>    .y3                0.012 #>    .y4               -0.009 #>  .y2 ~~                     #>    .y3               -0.006 #>    .y4                0.008 #>  .y3 ~~                     #>    .y4               -0.003 #>   ind60 ~~                  #>     dem60             0.391 #>  #> Variances: #>                    Estimate #>     ind60             1.000 #>    .x1                0.069 #>    .x2                0.143 #>    .x3                0.454 #>    .y1                2.193 #>    .y2                6.148 #>    .y3                5.246 #>    .y4                2.295 #>     dem60             1.000 pen_ests <- as.numeric(coef(pefa_fit2)[c(4:10, 15:35)]) sum(l0a(pen_ests)) #> [1] 1.564463"},{"path":"https://marklhc.github.io/plavaan/articles/penalized-fa.html","id":"penalize-cross-loadings-unique-covariances-and-difference-in-loadings-and-intercepts-across-time","dir":"Articles","previous_headings":"","what":"Penalize Cross-Loadings, Unique Covariances, and Difference in Loadings and Intercepts Across Time","title":"Penalized Estimation of Cross-Loadings and Unique Covariances","text":"First, model without cross-loadings concurrent unique covariances Parameter IDs: (Concurrent) Cross-loadings: 4 10 (Concurrent) Unique covariances: 35 55 Loadings across time: 11 18 Intercepts across time: 27 34 can compute “effective” number cross-loadings unique covariances non-zero: “effective” number loadings intercepts differ across time:","code":"mod3 <- \"     ind60 =~ NA * x1 + x2 + x3     dem60 =~ NA * l1 * y1 + l2 * y2 + l3 * y3 + l4 * y4     dem65 =~ NA * l1 * y5 + l2 * y6 + l3 * y7 + l4 * y8     dem60 ~ ind60     dem65 ~ ind60 + dem60     ind60 ~~ 1 * ind60     dem60 ~~ 1 * dem60     dem65 ~~ NA * dem65     ind60 ~ 0 * 1     dem60 ~ 0 * 1     dem65 ~ NA * 1     x1 + x2 + x3 ~ NA * 1     y1 ~ i1 * 1     y2 ~ i2 * 1     y3 ~ i3 * 1     y4 ~ i4 * 1     y5 ~ i1 * 1     y6 ~ i2 * 1     y7 ~ i3 * 1     y8 ~ i4 * 1     y1 ~~ y5     y2 ~~ y6     y3 ~~ y7     y4 ~~ y8 \" fit3_base <- cfa(mod3, data = PoliticalDemocracy) # Lavaan example of Political Democracy mod3_un <- \"     ind60 =~ NA * x1 + x2 + x3 + y1 + y2 + y3 + y4     dem60 =~ NA * x1 + x2 + x3 + y1 + y2 + y3 + y4     dem65 =~ NA * y5 + y6 + y7 + y8     dem60 ~ ind60     dem65 ~ ind60 + dem60     ind60 ~~ 1 * ind60     dem60 ~~ 1 * dem60     dem65 ~~ NA * dem65     ind60 ~ 0 * 1     dem60 ~ 0 * 1     dem65 ~ NA * 1     x1 + x2 + x3 + y1 + y2 + y3 + y4 ~ NA * 1     y5 + y6 + y7 + y8 ~ NA * 1     x1 ~~ x2 + x3 + y1 + y2 + y3 + y4     x2 ~~ x3 + y1 + y2 + y3 + y4     x3 ~~ y1 + y2 + y3 + y4     y1 ~~ y2 + y3 + y4     y2 ~~ y3 + y4     y3 ~~ y4     y1 ~~ y5     y2 ~~ y6     y3 ~~ y7     y4 ~~ y8 \" fit3 <- cfa(     mod3_un,     data = PoliticalDemocracy,     do.fit = FALSE,     start = fit3_base ) pt3 <- parTable(fit3) # Provide better starting values pt3$start[c(4:10, 35:55)] <- 0 fit3_2 <- lavaan::cfa(     pt3,     data = PoliticalDemocracy,     do.fit = FALSE ) pefa_fit3 <- penalized_est(     fit3_2,     w = .03,     pen_par_id = c(4:10, 35:55),     pen_diff_id = list(         loadings = rbind(11:14, 15:18),         intercepts = rbind(27:30, 31:34)     ) ) #> Warning in trans(x): NaNs produced summary(pefa_fit3, standardized = TRUE) #> lavaan 0.6-20 ended normally after 219 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        70 #>  #>   Number of observations                            75 #>  #>  #> Parameter Estimates: #>  #>  #> Latent Variables: #>                    Estimate   Std.lv  Std.all #>   ind60 =~                                    #>     x1                0.753    0.753    1.038 #>     x2                1.660    1.660    1.108 #>     x3                1.421    1.421    1.020 #>     y1               -0.001   -0.001   -0.000 #>     y2               -0.001   -0.001   -0.000 #>     y3                0.479    0.479    0.141 #>     y4                0.623    0.623    0.191 #>   dem60 =~                                    #>     x1                0.574    0.702    0.967 #>     x2                1.205    1.473    0.983 #>     x3                0.983    1.202    0.863 #>     y1                1.772    2.167    0.836 #>     y2                2.446    2.992    0.763 #>     y3                2.205    2.697    0.796 #>     y4                2.572    3.145    0.965 #>   dem65 =~                                    #>     y5                1.768    1.992    0.772 #>     y6                2.439    2.747    0.801 #>     y7                2.250    2.535    0.798 #>     y8                2.535    2.856    0.875 #>  #> Regressions: #>                    Estimate   Std.lv  Std.all #>   dem60 ~                                     #>     ind60            -0.704   -0.576   -0.576 #>   dem65 ~                                     #>     ind60             0.316    0.281    0.281 #>     dem60             1.005    1.091    1.091 #>  #> Covariances: #>                    Estimate   Std.lv  Std.all #>  .x1 ~~                                       #>    .x2                0.001    0.001    0.008 #>    .x3               -0.007   -0.007   -0.037 #>    .y1                0.024    0.024    0.062 #>    .y2               -0.040   -0.040   -0.057 #>    .y3                0.006    0.006    0.010 #>    .y4                0.015    0.015    0.034 #>  .x2 ~~                                       #>    .x3                0.001    0.001    0.004 #>    .y1               -0.009   -0.009   -0.016 #>    .y2                0.009    0.009    0.009 #>    .y3                0.006    0.006    0.007 #>    .y4               -0.010   -0.010   -0.017 #>  .x3 ~~                                       #>    .y1               -0.006   -0.006   -0.007 #>    .y2                0.002    0.002    0.001 #>    .y3               -0.010   -0.010   -0.007 #>    .y4                0.009    0.009    0.008 #>  .y1 ~~                                       #>    .y2               -0.008   -0.008   -0.002 #>    .y3                0.011    0.011    0.003 #>    .y4               -0.009   -0.009   -0.004 #>  .y2 ~~                                       #>    .y3               -0.004   -0.004   -0.001 #>    .y4                0.010    0.010    0.002 #>  .y3 ~~                                       #>    .y4                0.000    0.000    0.000 #>  .y1 ~~                                       #>    .y5                0.902    0.902    0.387 #>  .y2 ~~                                       #>    .y6                1.594    1.594    0.307 #>  .y3 ~~                                       #>    .y7                1.257    1.257    0.281 #>  .y4 ~~                                       #>    .y8                0.175    0.175    0.069 #>  #> Intercepts: #>                    Estimate   Std.lv  Std.all #>     ind60             0.000    0.000    0.000 #>    .dem60             0.000    0.000    0.000 #>    .dem65            -0.235   -0.208   -0.208 #>    .x1                5.072    5.072    6.990 #>    .x2                4.820    4.820    3.216 #>    .x3                3.584    3.584    2.572 #>    .y1                5.458    5.458    2.106 #>    .y2                3.764    3.764    0.960 #>    .y3                6.628    6.628    1.956 #>    .y4                4.503    4.503    1.381 #>    .y5                5.464    5.464    2.118 #>    .y6                3.748    3.748    1.093 #>    .y7                6.633    6.633    2.089 #>    .y8                4.510    4.510    1.381 #>  #> Variances: #>                    Estimate   Std.lv  Std.all #>     ind60             1.000    1.000    1.000 #>    .dem60             1.000    0.668    0.668 #>    .dem65             0.106    0.084    0.084 #>    .x1                0.076    0.076    0.144 #>    .x2                0.137    0.137    0.061 #>    .x3                0.445    0.445    0.229 #>    .y1                2.017    2.017    0.300 #>    .y2                6.413    6.413    0.417 #>    .y3                5.473    5.473    0.476 #>    .y4                2.601    2.601    0.245 #>    .y5                2.690    2.690    0.404 #>    .y6                4.211    4.211    0.358 #>    .y7                3.653    3.653    0.362 #>    .y8                2.503    2.503    0.235 pen_ests2 <- as.numeric(coef(pefa_fit3)[c(4:10, 35:55)]) sum(l0a(pen_ests2)) #> [1] 5.1989 ld_ests <- as.numeric(coef(pefa_fit3)[11:18]) int_ests <- as.numeric(coef(pefa_fit3)[27:34]) ld_mat <- matrix(ld_ests, nrow = 2, byrow = TRUE) int_mat <- matrix(int_ests, nrow = 2, byrow = TRUE) composite_pair_loss(ld_mat, fun = l0a) +     composite_pair_loss(int_mat, fun = l0a) #> [1] 0.3263673"},{"path":[]},{"path":"https://marklhc.github.io/plavaan/articles/standard-errors.html","id":"two-factor-cfa-model","dir":"Articles","previous_headings":"Penalize cross-loadings","what":"Two-factor CFA model","title":"Standard Errors","text":"Penalized","code":"mod0 <- \"   ind60 =~ x1 + x2 + x3   dem60 =~ y1 + y2 + y3 + y4   ind60 ~~ dem60 \" fit0 <- cfa(mod0, data = PoliticalDemocracy, std.lv = TRUE, estimator = \"MLR\") mod <- \"   ind60 =~ x1 + x2 + x3 + y1 + y2 + y3 + y4   dem60 =~ x1 + x2 + x3 + y1 + y2 + y3 + y4   ind60 ~~ ind60 \" fit <- cfa(mod, data = PoliticalDemocracy, std.lv = TRUE, do.fit = FALSE) pefa_fit <- penalized_est(     fit,     w = .03,     pen_par_id = 4:10,     se = \"robust.huber.white\" ) summary(pefa_fit) #> lavaan 0.6-20 ended normally after 126 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        22 #>  #>   Number of observations                            75 #>  #>  #> Parameter Estimates: #>  #>   Standard errors                             Sandwich #>   Information bread                           Observed #>   Observed information based on                Hessian #>  #> Latent Variables: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   ind60 =~                                             #>     x1                0.658    0.056   11.713    0.000 #>     x2                1.456    0.106   13.692    0.000 #>     x3                1.222    0.103   11.908    0.000 #>     y1               -0.007    0.008   -0.867    0.386 #>     y2               -0.608    0.476   -1.275    0.202 #>     y3               -0.001    0.006   -0.220    0.826 #>     y4                0.006    0.008    0.819    0.413 #>   dem60 =~                                             #>     x1                0.025    0.027    0.943    0.346 #>     x2               -0.002    0.014   -0.122    0.903 #>     x3               -0.010    0.015   -0.650    0.515 #>     y1                2.071    0.217    9.526    0.000 #>     y2                3.290    0.380    8.652    0.000 #>     y3                2.256    0.338    6.669    0.000 #>     y4                2.999    0.234   12.833    0.000 #>  #> Covariances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>   ind60 ~~                                             #>     dem60             0.481    0.107    4.475    0.000 #>  #> Variances: #>                    Estimate  Std.Err  z-value  P(>|z|) #>     ind60             1.000                            #>    .x1                0.079    0.018    4.352    0.000 #>    .x2                0.127    0.072    1.762    0.078 #>    .x3                0.464    0.082    5.651    0.000 #>    .y1                2.493    0.550    4.529    0.000 #>    .y2                6.048    1.370    4.415    0.000 #>    .y3                5.512    1.209    4.561    0.000 #>    .y4                2.017    0.635    3.177    0.001 #>     dem60             1.000 # Quick simulation to check SEs set.seed(1234) R <- 250 est_res <- matrix(NA, nrow = R, ncol = length(coef(pefa_fit))) se_res <- matrix(NA, nrow = R, ncol = length(coef(pefa_fit)))  # Use the simple structure model as population pop_model <- parTable(pefa_fit)  for (i in 1:R) {     # Simulate data     dat_sim <- simulateData(pop_model, sample.nobs = 200)      # Fit penalized model     fit_sim <- cfa(mod, data = dat_sim, std.lv = TRUE, do.fit = FALSE)     pefa_sim <- try(         penalized_est(             fit_sim,             w = .03,             pen_par_id = 4:10,             se = \"robust.huber.white\"         ),         silent = TRUE     )      if (!inherits(pefa_sim, \"try-error\")) {         est_res[i, ] <- coef(pefa_sim)         se_res[i, ] <- sqrt(diag(vcov(pefa_sim)))     } }  # Compare empirical SD vs mean SE for a few parameters # (e.g., first few loadings) res_summary <- data.frame(     param = names(coef(pefa_fit)),     emp_sd = apply(est_res, 2, sd, na.rm = TRUE),     mean_se = apply(se_res, 2, mean, na.rm = TRUE) ) print(res_summary, digits = 2) #>           param emp_sd mean_se #> 1     ind60=~x1 0.0380  0.0393 #> 2     ind60=~x2 0.0750  0.0772 #> 3     ind60=~x3 0.0787  0.0777 #> 4     ind60=~y1 0.0291  0.0060 #> 5     ind60=~y2 0.0704  0.0087 #> 6     ind60=~y3 0.0532  0.0069 #> 7     ind60=~y4 0.2631  0.1266 #> 8     dem60=~x1 0.0154  0.0152 #> 9     dem60=~x2 0.0093  0.0091 #> 10    dem60=~x3 0.0109  0.0106 #> 11    dem60=~y1 0.1586  0.1575 #> 12    dem60=~y2 0.2616  0.2445 #> 13    dem60=~y3 0.2160  0.2088 #> 14    dem60=~y4 0.2340  0.2057 #> 15       x1~~x1 0.0106  0.0116 #> 16       x2~~x2 0.0388  0.0436 #> 17       x3~~x3 0.0540  0.0536 #> 18       y1~~y1 0.3085  0.3133 #> 19       y2~~y2 0.8018  0.7797 #> 20       y3~~y3 0.5965  0.6039 #> 21       y4~~y4 0.4302  0.4333 #> 22 ind60~~dem60 0.0763  0.0686 # meat <- lavInspect(pefa_fit, \"information.first.order\") # bread <- attr(pefa_fit, \"hessian\") # vc_pefa <- solve(bread) %*% meat %*% solve(bread) / 75 # pefa_fit@vcov$vcov <- vc_pefa # se_pefa <- sqrt(diag(vc_pefa)) # pefa_fit@ParTable$se <- 0 * pefa_fit@ParTable$est # pefa_fit@ParTable$se[which(pefa_fit@ParTable$free > 0)] <- se_pefa # cbind(coef(pefa_fit), sqrt(diag(vc_pefa)))"},{"path":"https://marklhc.github.io/plavaan/articles/standard-errors.html","id":"penalize-non-invariance","dir":"Articles","previous_headings":"","what":"Penalize non-invariance","title":"Standard Errors","text":"Compared scalar invariance model","code":"lconfig_mod_un <- \"     # Time 1     dem60 =~ .l1_1 * y1 + .l2_1 * y2 + .l3_1 * y3 + .l4_1 * y4     y1 ~ .i1_1 * 1     y2 ~ .i2_1 * 1     y3 ~ .i3_1 * 1     y4 ~ .i4_1 * 1     y1 ~~ .u1_1 * y1     y2 ~~ .u2_1 * y2     y3 ~~ .u3_1 * y3     y4 ~~ .u4_1 * y4          # Time 2     dem65 =~ .l1_2 * y5 + .l2_2 * y6 + .l3_2 * y7 + .l4_2 * y8     y5 ~ .i1_2 * 1     y6 ~ .i2_2 * 1     y7 ~ .i3_2 * 1     y8 ~ .i4_2 * 1     y5 ~~ .u1_2 * y5     y6 ~~ .u2_2 * y6     y7 ~~ .u3_2 * y7     y8 ~~ .u4_2 * y8          # Latent variances     dem60 ~~ 1 * dem60     dem65 ~~ NA * dem65          # Latent means     dem60 ~ 0 * 1     dem65 ~ NA * 1          # Lag Covariances     y1 ~~ y5     y2 ~~ y6     y3 ~~ y7     y4 ~~ y8 \" # Specify the under-identified model lconfig_fit_un <- cfa(     lconfig_mod_un,     data = PoliticalDemocracy,     do.fit = FALSE,     std.lv = TRUE,     missing = \"fiml\",     estimator = \"mlr\" ) ld_id <- rbind(1:4, 13:16) int_id <- rbind(5:8, 17:20) pen_fit <- penalized_est(     lconfig_fit_un,     w = 0.03,     pen_fn = \"l0a\",     pen_diff_id = list(loadings = ld_id, intercepts = int_id),     se = \"robust.huber.white\" ) parameterEstimates(pen_fit) #>      lhs op   rhs label    est    se      z pvalue ci.lower ci.upper #> 1  dem60 =~    y1 .l1_1  2.105 0.207 10.182  0.000    1.700    2.510 #> 2  dem60 =~    y2 .l2_1  2.852 0.290  9.828  0.000    2.283    3.420 #> 3  dem60 =~    y3 .l3_1  2.531 0.241 10.512  0.000    2.059    3.003 #> 4  dem60 =~    y4 .l4_1  2.905 0.226 12.872  0.000    2.462    3.347 #> 5     y1 ~1       .i1_1  5.456 0.288 18.918  0.000    4.890    6.021 #> 6     y2 ~1       .i2_1  4.251 0.453  9.389  0.000    3.364    5.139 #> 7     y3 ~1       .i3_1  6.572 0.355 18.513  0.000    5.876    7.268 #> 8     y4 ~1       .i4_1  4.460 0.366 12.173  0.000    3.742    5.178 #> 9     y1 ~~    y1 .u1_1  2.129 0.487  4.370  0.000    1.174    3.084 #> 10    y2 ~~    y2 .u2_1  6.632 1.319  5.030  0.000    4.048    9.217 #> 11    y3 ~~    y3 .u3_1  5.388 1.095  4.921  0.000    3.242    7.534 #> 12    y4 ~~    y4 .u4_1  2.594 0.643  4.033  0.000    1.333    3.854 #> 13 dem65 =~    y5 .l1_2  2.083 0.210  9.929  0.000    1.672    2.494 #> 14 dem65 =~    y6 .l2_2  2.819 0.290  9.710  0.000    2.250    3.388 #> 15 dem65 =~    y7 .l3_2  2.602 0.252 10.322  0.000    2.108    3.096 #> 16 dem65 =~    y8 .l4_2  2.898 0.225 12.905  0.000    2.458    3.339 #> 17    y5 ~1       .i1_2  5.454 0.288 18.945  0.000    4.890    6.019 #> 18    y6 ~1       .i2_2  3.393 0.428  7.937  0.000    2.555    4.231 #> 19    y7 ~1       .i3_2  6.572 0.355 18.490  0.000    5.876    7.269 #> 20    y8 ~1       .i4_2  4.460 0.366 12.182  0.000    3.743    5.178 #> 21    y5 ~~    y5 .u1_2  2.816 0.591  4.769  0.000    1.659    3.974 #> 22    y6 ~~    y6 .u2_2  4.003 0.803  4.986  0.000    2.429    5.576 #> 23    y7 ~~    y7 .u3_2  3.590 0.637  5.640  0.000    2.342    4.838 #> 24    y8 ~~    y8 .u4_2  2.457 0.721  3.409  0.001    1.044    3.870 #> 25 dem60 ~~ dem60        1.000 0.000     NA     NA    1.000    1.000 #> 26 dem65 ~~ dem65        0.951 0.097  9.812  0.000    0.761    1.141 #> 27 dem60 ~1              0.000 0.000     NA     NA    0.000    0.000 #> 28 dem65 ~1             -0.147 0.070 -2.091  0.037   -0.284   -0.009 #> 29    y1 ~~    y5        0.842 0.433  1.943  0.052   -0.007    1.692 #> 30    y2 ~~    y6        1.820 0.880  2.068  0.039    0.095    3.545 #> 31    y3 ~~    y7        1.222 0.644  1.898  0.058   -0.040    2.483 #> 32    y4 ~~    y8        0.284 0.467  0.608  0.543   -0.632    1.201 #> 33 dem60 ~~ dem65        0.918 0.057 16.132  0.000    0.806    1.029 lscalar_mod <- \"     # Time 1     dem60 =~ .l1 * y1 + .l2 * y2 + .l3 * y3 + .l4 * y4     y1 ~ .i1 * 1     y2 ~ .i2 * 1     y3 ~ .i3 * 1     y4 ~ .i4 * 1     y1 ~~ .u1_1 * y1     y2 ~~ .u2_1 * y2     y3 ~~ .u3_1 * y3     y4 ~~ .u4_1 * y4          # Time 2     dem65 =~ .l1 * y5 + .l2 * y6 + .l3 * y7 + .l4 * y8     y5 ~ .i1 * 1     y6 ~ .i2 * 1     y7 ~ .i3 * 1     y8 ~ .i4 * 1     y5 ~~ .u1_2 * y5     y6 ~~ .u2_2 * y6     y7 ~~ .u3_2 * y7     y8 ~~ .u4_2 * y8          # Latent variances     dem60 ~~ 1 * dem60     dem65 ~~ NA * dem65          # Latent means     dem60 ~ 0 * 1     dem65 ~ NA * 1          # Lag Covariances     y1 ~~ y5     y2 ~~ y6     y3 ~~ y7     y4 ~~ y8 \" lscalar_fit <- cfa(     lscalar_mod,     data = PoliticalDemocracy,     std.lv = TRUE,     missing = \"fiml\",     estimator = \"mlr\" ) parameterEstimates(lscalar_fit) #>      lhs op   rhs label    est    se      z pvalue ci.lower ci.upper #> 1  dem60 =~    y1   .l1  2.085 0.211  9.884  0.000    1.672    2.498 #> 2  dem60 =~    y2   .l2  2.896 0.284 10.189  0.000    2.339    3.453 #> 3  dem60 =~    y3   .l3  2.552 0.246 10.366  0.000    2.069    3.034 #> 4  dem60 =~    y4   .l4  2.889 0.225 12.842  0.000    2.448    3.330 #> 5     y1 ~1         .i1  5.508 0.289 19.037  0.000    4.941    6.075 #> 6     y2 ~1         .i2  3.796 0.429  8.856  0.000    2.956    4.636 #> 7     y3 ~1         .i3  6.670 0.353 18.915  0.000    5.979    7.361 #> 8     y4 ~1         .i4  4.554 0.366 12.458  0.000    3.838    5.270 #> 9     y1 ~~    y1 .u1_1  2.141 0.488  4.384  0.000    1.184    3.098 #> 10    y2 ~~    y2 .u2_1  6.840 1.444  4.738  0.000    4.010    9.669 #> 11    y3 ~~    y3 .u3_1  5.424 1.101  4.927  0.000    3.266    7.582 #> 12    y4 ~~    y4 .u4_1  2.623 0.641  4.091  0.000    1.366    3.879 #> 13 dem65 =~    y5   .l1  2.085 0.211  9.884  0.000    1.672    2.498 #> 14 dem65 =~    y6   .l2  2.896 0.284 10.189  0.000    2.339    3.453 #> 15 dem65 =~    y7   .l3  2.552 0.246 10.366  0.000    2.069    3.034 #> 16 dem65 =~    y8   .l4  2.889 0.225 12.842  0.000    2.448    3.330 #> 17    y5 ~1         .i1  5.508 0.289 19.037  0.000    4.941    6.075 #> 18    y6 ~1         .i2  3.796 0.429  8.856  0.000    2.956    4.636 #> 19    y7 ~1         .i3  6.670 0.353 18.915  0.000    5.979    7.361 #> 20    y8 ~1         .i4  4.554 0.366 12.458  0.000    3.838    5.270 #> 21    y5 ~~    y5 .u1_2  2.824 0.577  4.895  0.000    1.693    3.954 #> 22    y6 ~~    y6 .u2_2  4.032 0.818  4.931  0.000    2.429    5.634 #> 23    y7 ~~    y7 .u3_2  3.650 0.639  5.713  0.000    2.398    4.903 #> 24    y8 ~~    y8 .u4_2  2.482 0.705  3.522  0.000    1.101    3.863 #> 25 dem60 ~~ dem60        1.000 0.000     NA     NA    1.000    1.000 #> 26 dem65 ~~ dem65        0.947 0.097  9.745  0.000    0.756    1.137 #> 27 dem60 ~1              0.000 0.000     NA     NA    0.000    0.000 #> 28 dem65 ~1             -0.210 0.067 -3.115  0.002   -0.342   -0.078 #> 29    y1 ~~    y5        0.845 0.433  1.953  0.051   -0.003    1.694 #> 30    y2 ~~    y6        1.670 0.934  1.788  0.074   -0.161    3.502 #> 31    y3 ~~    y7        1.206 0.646  1.868  0.062   -0.060    2.472 #> 32    y4 ~~    y8        0.261 0.465  0.563  0.574   -0.649    1.172 #> 33 dem60 ~~ dem65        0.918 0.057 16.191  0.000    0.807    1.030"},{"path":"https://marklhc.github.io/plavaan/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Hok Chio (Mark) Lai. Author, maintainer, copyright holder.","code":""},{"path":"https://marklhc.github.io/plavaan/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Lai H (2025). plavaan: Penalized estimation latent variable models 'lavaan'. R package version 0.0.1, https://marklhc.github.io/plavaan/.","code":"@Manual{,   title = {plavaan: Penalized estimation for latent variable models with 'lavaan'},   author = {Hok Chio (Mark) Lai},   year = {2025},   note = {R package version 0.0.1},   url = {https://marklhc.github.io/plavaan/}, }"},{"path":"https://marklhc.github.io/plavaan/index.html","id":"plavaan","dir":"","previous_headings":"","what":"Penalized estimation for latent variable models with lavaan","title":"Penalized estimation for latent variable models with lavaan","text":"goal plavaan perform penalized estimation latent variable models using lavaan differentiable penalty function, described Robitzsch (2023) similar approach described Asparouhov & Muthén (2024).","code":""},{"path":"https://marklhc.github.io/plavaan/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Penalized estimation for latent variable models with lavaan","text":"can install development version plavaan like :","code":"# install.packages(\"remotes\") remotes::install_github(\"marklhc/plavaan\")"},{"path":"https://marklhc.github.io/plavaan/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"Penalized estimation for latent variable models with lavaan","text":"basic example obtain penalized cross-loadings confirmatory factor analysis model: Sandwich estimates standard errors can obtained using se = \"robust.huber.white\" argument penalized_est(), unclear valid presence penalization.","code":"library(lavaan) #> This is lavaan 0.6-20 #> lavaan is FREE software! Please report any bugs. library(plavaan) data(HolzingerSwineford1939) model <- '   visual =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9   textual =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9   speed =~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9 '  # Dry-run to get starting values for the initial un-identified model fit_dry <- cfa(model, data = HolzingerSwineford1939, std.lv = TRUE, do.fit = FALSE)  # Locate the cross-loadings to be penalized from the parameter table pt <- parTable(fit_dry) subset(pt, op == \"=~\") #>    id     lhs op rhs user block group free ustart exo label plabel start   est #> 1   1  visual =~  x1    1     1     1    1     NA   0         .p1. 0.815 0.815 #> 2   2  visual =~  x2    1     1     1    2     NA   0         .p2. 0.515 0.515 #> 3   3  visual =~  x3    1     1     1    3     NA   0         .p3. 0.592 0.592 #> 4   4  visual =~  x4    1     1     1    4     NA   0         .p4. 0.680 0.680 #> 5   5  visual =~  x5    1     1     1    5     NA   0         .p5. 0.769 0.769 #> 6   6  visual =~  x6    1     1     1    6     NA   0         .p6. 0.687 0.687 #> 7   7  visual =~  x7    1     1     1    7     NA   0         .p7. 0.315 0.315 #> 8   8  visual =~  x8    1     1     1    8     NA   0         .p8. 0.335 0.335 #> 9   9  visual =~  x9    1     1     1    9     NA   0         .p9. 0.521 0.521 #> 10 10 textual =~  x1    1     1     1   10     NA   0        .p10. 0.815 0.815 #> 11 11 textual =~  x2    1     1     1   11     NA   0        .p11. 0.515 0.515 #> 12 12 textual =~  x3    1     1     1   12     NA   0        .p12. 0.592 0.592 #> 13 13 textual =~  x4    1     1     1   13     NA   0        .p13. 0.680 0.680 #> 14 14 textual =~  x5    1     1     1   14     NA   0        .p14. 0.769 0.769 #> 15 15 textual =~  x6    1     1     1   15     NA   0        .p15. 0.687 0.687 #> 16 16 textual =~  x7    1     1     1   16     NA   0        .p16. 0.315 0.315 #> 17 17 textual =~  x8    1     1     1   17     NA   0        .p17. 0.335 0.335 #> 18 18 textual =~  x9    1     1     1   18     NA   0        .p18. 0.521 0.521 #> 19 19   speed =~  x1    1     1     1   19     NA   0        .p19. 0.815 0.815 #> 20 20   speed =~  x2    1     1     1   20     NA   0        .p20. 0.515 0.515 #> 21 21   speed =~  x3    1     1     1   21     NA   0        .p21. 0.592 0.592 #> 22 22   speed =~  x4    1     1     1   22     NA   0        .p22. 0.680 0.680 #> 23 23   speed =~  x5    1     1     1   23     NA   0        .p23. 0.769 0.769 #> 24 24   speed =~  x6    1     1     1   24     NA   0        .p24. 0.687 0.687 #> 25 25   speed =~  x7    1     1     1   25     NA   0        .p25. 0.315 0.315 #> 26 26   speed =~  x8    1     1     1   26     NA   0        .p26. 0.335 0.335 #> 27 27   speed =~  x9    1     1     1   27     NA   0        .p27. 0.521 0.521  # Fit the penalized model using plavaan::penalized_est() fit_pen <- penalized_est(   fit_dry,   w = 0.01,  # close to a BIC-like weight for penalty   pen_par_id = c(4:12, 16:24),  # cross-loadings id in the free column   pen_fn = \"l0a\"  # approximate L0 penalty ) summary(fit_pen) #> lavaan 0.6-20 ended normally after 113 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        39 #>  #>   Number of observations                           301 #>  #>  #> Parameter Estimates: #>  #>  #> Latent Variables: #>                    Estimate #>   visual =~                 #>     x1                0.813 #>     x2                0.545 #>     x3                0.825 #>     x4                0.015 #>     x5               -0.057 #>     x6                0.036 #>     x7               -0.043 #>     x8                0.034 #>     x9                0.382 #>   textual =~                #>     x1                0.029 #>     x2               -0.012 #>     x3               -0.205 #>     x4                0.975 #>     x5                1.137 #>     x6                0.889 #>     x7                0.016 #>     x8               -0.009 #>     x9               -0.010 #>   speed =~                  #>     x1                0.011 #>     x2               -0.028 #>     x3                0.007 #>     x4               -0.004 #>     x5                0.003 #>     x6                0.002 #>     x7                0.698 #>     x8                0.773 #>     x9                0.460 #>  #> Covariances: #>                    Estimate #>   visual ~~                 #>     textual           0.481 #>     speed             0.282 #>   textual ~~                #>     speed             0.203 #>  #> Variances: #>                    Estimate #>    .x1                0.658 #>    .x2                1.091 #>    .x3                0.701 #>    .x4                0.378 #>    .x5                0.412 #>    .x6                0.364 #>    .x7                0.701 #>    .x8                0.408 #>    .x9                0.558 #>     visual            1.000 #>     textual           1.000 #>     speed             1.000"},{"path":"https://marklhc.github.io/plavaan/reference/composite_pair_loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Composite Pairwise Loss Function — composite_pair_loss","title":"Composite Pairwise Loss Function — composite_pair_loss","text":"Computes total loss across pairwise combinations rows matrix.","code":""},{"path":"https://marklhc.github.io/plavaan/reference/composite_pair_loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Composite Pairwise Loss Function — composite_pair_loss","text":"","code":"composite_pair_loss(x, fun, trans = identity, rescale = \"df\", ...)"},{"path":"https://marklhc.github.io/plavaan/reference/composite_pair_loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Composite Pairwise Loss Function — composite_pair_loss","text":"x numeric vector, matrix, data frame. matrix, coerced one applying transformation function. fun function compute loss pairwise difference. package supports alignment loss (alf) approximate L0 penalty (l0a), users can provide custom functions well. trans transformation function apply x computing pairwise differences. Default identity (transformation). rescale Either \"df\" (default) rescale total loss degrees freedom (number rows - 1), numeric value (likely 0 1) multiply total loss . ... Additional arguments passed loss function fun.","code":""},{"path":"https://marklhc.github.io/plavaan/reference/composite_pair_loss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Composite Pairwise Loss Function — composite_pair_loss","text":"numeric scalar representing sum losses across pairwise combinations rows.","code":""},{"path":"https://marklhc.github.io/plavaan/reference/composite_pair_loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Composite Pairwise Loss Function — composite_pair_loss","text":"function works : Applying transformation function trans input x Converting result matrix Generating possible pairwise combinations row indices Computing difference pair rows Applying loss function fun difference Summing individual losses","code":""},{"path":"https://marklhc.github.io/plavaan/reference/composite_pair_loss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Composite Pairwise Loss Function — composite_pair_loss","text":"","code":"# Example with a simple matrix x <- matrix(runif(12), nrow = 4) composite_pair_loss(x, fun = alf) #> [1] 5.148134  # Example with log transformation and L2 loss composite_pair_loss(x, fun = function(x) x^2, trans = log) #> [1] 36.73551"},{"path":"https://marklhc.github.io/plavaan/reference/loss.html","id":null,"dir":"Reference","previous_headings":"","what":"Loss functions — loss","title":"Loss functions — loss","text":"small eps provides smooth, numerically stable approximation |x|^(1/2) (.e. square root absolute value). function vectorized x.","code":""},{"path":"https://marklhc.github.io/plavaan/reference/loss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Loss functions — loss","text":"","code":"alf(x, eps = 0.001)  l0a(x, eps = 0.01)"},{"path":"https://marklhc.github.io/plavaan/reference/loss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Loss functions — loss","text":"x Numeric vector. Input values transform. eps Positive numeric scalar (default .001 alf() .01 l0a()). Small regularization constant avoid non-differentiability division--zero issues.","code":""},{"path":"https://marklhc.github.io/plavaan/reference/loss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Loss functions — loss","text":"Numeric vector length x.","code":""},{"path":"https://marklhc.github.io/plavaan/reference/loss.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Loss functions — loss","text":"ALF, (x^2 + eps)^(1/4), useful smooth surrogate sqrt(|x|) required (optimization regularization) maintaining numerical stability near x = 0. L0a, x^2/(x^2 + eps), approximation L0 penalty.","code":""},{"path":"https://marklhc.github.io/plavaan/reference/loss.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Loss functions — loss","text":"","code":"alf(0) #> [1] 0.1778279 alf(c(-4, -1, 0, 1, 4)) #> [1] 2.0000312 1.0002499 0.1778279 1.0002499 2.0000312 alf(0.5, eps = 1e-6) #> [1] 0.7071075 l0a(0) #> [1] 0 l0a(c(0, 1e-3, 0.1, 1)) #> [1] 0.00000000 0.00009999 0.50000000 0.99009901 l0a(c(-2, 0, 2), eps = 1e-4) #> [1] 0.999975 0.000000 0.999975"},{"path":"https://marklhc.github.io/plavaan/reference/penalized_est.html","id":null,"dir":"Reference","previous_headings":"","what":"Penalized Parameter Estimation for Longitudinal CFA Models — penalized_est","title":"Penalized Parameter Estimation for Longitudinal CFA Models — penalized_est","text":"Performs penalized estimation lavaan model object optimizing penalized objective function. function extracts objective function lavaan model, applies penalty function specified parameters pairwise differences parameters, returns updated model optimized parameter estimates.","code":""},{"path":"https://marklhc.github.io/plavaan/reference/penalized_est.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Penalized Parameter Estimation for Longitudinal CFA Models — penalized_est","text":"","code":"penalized_est(   x,   w,   pen_par_id = NULL,   pen_diff_id = NULL,   pen_fn = \"l0a\",   pen_gr = NULL,   se = \"none\",   opt_control = list() )"},{"path":"https://marklhc.github.io/plavaan/reference/penalized_est.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Penalized Parameter Estimation for Longitudinal CFA Models — penalized_est","text":"x fitted lavaan model object estimation components extracted. w Numeric scalar. Penalty weight (multiplier) applied penalty terms. pen_par_id Integer vector parameter IDs apply penalty function directly , order returned lavaan::coef() lavaan::partable(), free elements. pen_diff_id List matrices containing parameter IDs. matrix, penalty applied pairwise differences parameters column indicated IDs. matrices names starting \"loading\", log transformation applied computing differences. pen_fn character string (\"l0a\" \"alf\") function computes penalty. Default \"l0a\". pen_gr function computes gradient penalty function. pen_fn \"l0a\" \"alf\", automatically set. se Character string specifying type standard errors compute. Options \"none\" (default; standard errors) \"robust.huber.white\" (robust sandwich estimator using numerical Hessian first-order information, used \"mlr\" estimator). opt_control list control parameters passed stats::nlminb(). Default includes eval.max = 2e4, iter.max = 1e4, abs.tol = 1e-20.","code":""},{"path":"https://marklhc.github.io/plavaan/reference/penalized_est.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Penalized Parameter Estimation for Longitudinal CFA Models — penalized_est","text":"lavaan model object updated penalized parameter estimates. returned object includes attribute opt_info containing optimization information returned nlminb().","code":""},{"path":"https://marklhc.github.io/plavaan/reference/penalized_est.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Penalized Parameter Estimation for Longitudinal CFA Models — penalized_est","text":"function uses nlminb() minimize penalized objective function combines standard lavaan objective function penalty term. parameter estimates log-likelihood interpreted. returned object \"fitted\" (.fit = FALSE) avoid users interpreting standard errors, generally valid penalized estimation. degrees freedom may also inaccurate. optimization converge (convergence code != 0), warning issued.","code":""},{"path":"https://marklhc.github.io/plavaan/reference/penalized_est.html","id":"warning","dir":"Reference","previous_headings":"","what":"Warning","title":"Penalized Parameter Estimation for Longitudinal CFA Models — penalized_est","text":"returned object fitted using standard ML. Standard errors reported summary() parameterEstimates() missing unless se = \"robust.huber.white\" specified. Even , based experimental sandwich approximation interpreted caution.","code":""},{"path":[]},{"path":"https://marklhc.github.io/plavaan/reference/penalized_est.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Penalized Parameter Estimation for Longitudinal CFA Models — penalized_est","text":"","code":"library(lavaan) #> This is lavaan 0.6-20 #> lavaan is FREE software! Please report any bugs.  # Define a longitudinal factor model with PoliticalDemocracy data model <- \"   dem60 =~ y1 + y2 + y3 + y4   dem65 =~ y5 + y6 + y7 + y8   dem60 ~~ dem65   dem60 ~~ 1 * dem60   dem65 ~~ NA * dem65   dem60 ~ 0   dem65 ~ NA * 1   y1 ~~ y5   y2 ~~ y6   y3 ~~ y7   y4 ~~ y8 \"  # Fit the model without constraints first to get parameter table fit_un <- cfa(model, data = PoliticalDemocracy, std.lv = TRUE,               meanstructure = TRUE, do.fit = FALSE)  # Get parameter IDs pt <- parTable(fit_un) # Loadings load_60 <- pt$free[pt$op == \"=~\" & pt$lhs == \"dem60\"] load_65 <- pt$free[pt$op == \"=~\" & pt$lhs == \"dem65\"] # Intercepts int_60 <- pt$free[pt$op == \"~1\" & pt$lhs %in% c(\"y1\", \"y2\", \"y3\", \"y4\")] int_65 <- pt$free[pt$op == \"~1\" & pt$lhs %in% c(\"y5\", \"y6\", \"y7\", \"y8\")]  # Apply penalized estimation to penalize differences in loadings and intercepts pen_fit <- penalized_est(     x = fit_un,     w = 0.03,     pen_diff_id = list(         loadings = rbind(load_60, load_65),         intercepts = rbind(int_60, int_65)     ),     pen_fn = \"l0a\" )  # Compare parameter estimates summary(pen_fit) #> lavaan 0.6-20 ended normally after 103 iterations #>  #>   Estimator                                         ML #>   Optimization method                           NLMINB #>   Number of model parameters                        31 #>  #>   Number of observations                            75 #>  #>  #> Parameter Estimates: #>  #>  #> Latent Variables: #>                    Estimate #>   dem60 =~                  #>     y1                2.105 #>     y2                2.852 #>     y3                2.531 #>     y4                2.905 #>   dem65 =~                  #>     y5                2.083 #>     y6                2.819 #>     y7                2.602 #>     y8                2.898 #>  #> Covariances: #>                    Estimate #>   dem60 ~~                  #>     dem65             0.918 #>  .y1 ~~                     #>    .y5                0.842 #>  .y2 ~~                     #>    .y6                1.820 #>  .y3 ~~                     #>    .y7                1.222 #>  .y4 ~~                     #>    .y8                0.284 #>  #> Intercepts: #>                    Estimate #>     dem60             0.000 #>     dem65            -0.147 #>    .y1                5.456 #>    .y2                4.251 #>    .y3                6.572 #>    .y4                4.460 #>    .y5                5.454 #>    .y6                3.393 #>    .y7                6.572 #>    .y8                4.460 #>  #> Variances: #>                    Estimate #>     dem60             1.000 #>     dem65             0.951 #>    .y1                2.129 #>    .y2                6.632 #>    .y3                5.388 #>    .y4                2.594 #>    .y5                2.816 #>    .y6                4.003 #>    .y7                3.590 #>    .y8                2.457 #>"},{"path":"https://marklhc.github.io/plavaan/news/index.html","id":"plavaan-001","dir":"Changelog","previous_headings":"","what":"plavaan 0.0.1","title":"plavaan 0.0.1","text":"Initial CRAN submission.","code":""}]
